{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de ACV - Aprendizaje de Máquina 1\n",
    "\n",
    "Autores: María Fabiana Cid y Gonzalo Gabriel Fernandez, Universidad de Buenos Aires\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Según la Organización Mundial de la Salud (OMS), el accidente cerebrovascular (ACV) es la segunda causa principal de muerte a nivel mundial, siendo responsable de aproximadamente el 11% del total de muertes.\n",
    "\n",
    "En el presente trabajo se analiza el conjunto de datos titulado [Stroke Prediction](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) publicado en Kaggle.\n",
    "\n",
    "Se plantea como objetivo **predecir**, conociendo determinadas caracteristicas fisiológicas y sociales del paciente, la posibilidad de que tenga un accidente cardiovascular.\n",
    "\n",
    "Primero se realiza la recolección de datos y su preparación para ser analizados. Luego, se realiza una ingenieria de features, el entrenamiento de distintos modelos y su evaluación.\n",
    "\n",
    "De todos los algoritmos clásicos de aprendizaje de máquina estudiados, el de mejor desempeño fue el Random Forest con los siguiente indicadores:\n",
    "\n",
    "- Accuracy: 0.964524\n",
    "- Precisión: 0.961105\n",
    "- Recall: 0.968041\n",
    "- F1-score: 0.964561\n",
    "\n",
    "Además, se entrenaron modelos de redes neuronales (aprendizaje profundo con las librerias Torch y Tensorflow). También se incorporaron herramientas de aprendizaje no supervisado, como Tsne para reducción de dimensionalidad y clustering con K-means.\n",
    "\n",
    "### Información de los atributos del dataset utilizado\n",
    "\n",
    "- id: identificador único\n",
    "- gender: \"Male\" (Hombre), \"Female\" (Mujer) o \"Other\" (Otro)\n",
    "- age: edad del paciente\n",
    "- hypertension: 0 si el paciente no tiene hipertensión, 1 si el paciente tiene hipertensión\n",
    "- heart_disease: 0 si el paciente no tiene enfermedades cardíacas, 1 si el paciente tiene una enfermedad cardíaca\n",
    "- ever_married: \"No\" o \"Yes\" (No o Sí)\n",
    "- work_type: \"children\" (niños), \"Govt_job\" (empleo gubernamental), \"Never_worked\" (nunca trabajó), \"Private\" (sector privado) o \"Self-employed\" (autónomo)\n",
    "- Residence_type: \"Rural\" o \"Urban\" (Rural o Urbano)\n",
    "- avg_glucose_level: nivel promedio de glucosa en sangre\n",
    "- bmi: índice de masa corporal\n",
    "- smoking_status: \"formerly smoked\" (fumó anteriormente), \"never smoked\" (nunca fumó), \"smokes\" (fuma) o \"Unknown\" (desconocido)\n",
    "- stroke: 1 si el paciente sufrió un accidente cerebrovascular, 0 si no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencias\n",
    "\n",
    "Para modelos de aprendizaje de máquina clásicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para redes neuronales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feed-forward neural network\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "import torch.nn\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el dataset \"Stroke Prediction\". Se encuentra disponible el script `download_dataset.sh` para su descarga desde Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "df.set_index(\"id\", inplace=True)\n",
    "print(\"Número de observaciones:\", len(df))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación de datos duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de datos duplicados:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación de datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que la única variable con valores nulos es *bmi*.\n",
    "\n",
    "Se asume que la razón por la cual faltan datos es completamente aleatoria (MCAR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_nan = df.dropna(axis=\"index\", how=\"any\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "axs[0].set_title(\"BMI NO NAN\")\n",
    "sns.countplot(data=df_no_nan, x=\"stroke\", ax=axs[0])\n",
    "axs[1].set_title(\"BMI NAN\")\n",
    "sns.countplot(data=df[df.isnull().any(axis=1)], x=\"stroke\", ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por el momento, se eliminan las filas con valores faltantes de bmi. Si en el modelo no se utiliza la variable bmi, es importante no eliminar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=\"index\", how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de distribución de los datos\n",
    "#### Variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    \"work_type\",\n",
    "    \"smoking_status\",\n",
    "    \"gender\",\n",
    "    \"Residence_type\",\n",
    "    \"ever_married\",\n",
    "    \"heart_disease\",\n",
    "    \"hypertension\",\n",
    "]\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(LABELS))\n",
    "fig.set_size_inches(len(LABELS) * 10, 10)\n",
    "for ax, label in zip(axs, LABELS):\n",
    "    sns.countplot(df, x=label, hue=\"stroke\", stat=\"percent\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
    "fig, axs = plt.subplots(nrows=2, ncols=len(LABELS))\n",
    "fig.set_size_inches(len(LABELS) * 10, 10)\n",
    "for ax_hist, ax_box, label in zip(axs[0], axs[1], LABELS):\n",
    "    sns.histplot(df, x=label, hue=\"stroke\", ax=ax_hist)\n",
    "    sns.boxplot(df, x=label, hue=\"stroke\", ax=ax_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En la distribución de la edad se puede observar como hay una claro aumento en derrames cerebrales al aumentar la edad. Por el otro lado, el nivel de glucosa y indice de masa corporal paece tener la misma distribución para casos con y sin derrame cerebral.\n",
    "- En la distribución del índice de masa corporal se observa una gran cantidad de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    df.drop([\"hypertension\", \"heart_disease\"], axis=1).sort_values(\"stroke\"),\n",
    "    hue=\"stroke\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de correlación de las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = df.select_dtypes(include=[np.number])\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Map de calor con correlación de variables\")\n",
    "sns.heatmap(number.corr(), annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar una gran correlación entre:\n",
    "- el índice de masa corporal y la presencia enfermedades cardíacas.\n",
    "- el índice de masa corporal y la hipertensión.\n",
    "- el índice de masa corporal y la presencia de derrame cerebral.\n",
    "- la edad no parece tener una gran correlación con el resto de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de distribución de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de los datos\n",
    "Transformación de variables categóricas. Posible mediante **One-Hot encoding** dada que la cantidad de categorías es relativamente baja y con poca probabilidad de cambiar/extenderse en el futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cathegoric variables transformation\n",
    "print(\"Applying One-Hot encoding to:\")\n",
    "for label, to_drop in [\n",
    "    (\"gender\", \"Other\"),\n",
    "    (\"ever_married\", \"No\"),\n",
    "    (\"work_type\", \"children\"),\n",
    "    (\"Residence_type\", \"Rural\"),\n",
    "    (\"smoking_status\", \"Unknown\"),\n",
    "]:\n",
    "    unique_values = df[label].unique()\n",
    "    print(label + \":\", unique_values)\n",
    "    prefix = \"is\" if len(unique_values) > 2 else label\n",
    "    one_hot = pd.get_dummies(data=df[label], prefix=prefix).drop(\n",
    "        prefix + \"_\" + to_drop if prefix else to_drop, axis=1\n",
    "    )\n",
    "    df.drop(label, axis=1, inplace=True)\n",
    "    df = df.join(one_hot)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clases imbalanceadas: De [Datasets: Imbalanced datasets](https://developers.google.com/machine-learning/crash-course/overfitting/imbalanced-datasets), el grado de imbalance es **moderado**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_count = df.stroke.value_counts()\n",
    "print(stroke_count, len(df))\n",
    "print(f\"Degree of imbalance: {stroke_count[1] / len(df) * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de diferentes modelos ignorando desbalanceo\n",
    "Separación del dataset en variable target y variables de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"stroke\", axis=1).values\n",
    "y = df[\"stroke\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalamiento de las variables de entrada. `RobustScaler` elimina la mediana y escala según el rango intercuartílico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "División en set de entrenamiento y de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, los modelos a entrenar siguiendo el orden en que se estudiaron en la materia \"Aprendizaje de Máquina I\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokePredictionModel:\n",
    "    \"\"\"Machine learning model for stroke prediction\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, model: ClassifierMixin) -> None:\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.accuracy = 0.0\n",
    "        self.precision = 0.0\n",
    "        self.recall = 0.0\n",
    "        self.f1_score = 0.0\n",
    "        self.confusion_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\n",
    "    StrokePredictionModel(\n",
    "        name=\"Regresión Logística\", model=LogisticRegression(max_iter=1000)\n",
    "    ),\n",
    "    StrokePredictionModel(\n",
    "        name=\"Clasificador KNN\", model=KNeighborsClassifier(n_neighbors=3)\n",
    "    ),\n",
    "    StrokePredictionModel(\n",
    "        name=\"Support Vector Machine\", model=SVC(kernel=\"rbf\", C=0.01, gamma=0.01)\n",
    "    ),\n",
    "    StrokePredictionModel(\n",
    "        name=\"Árbol de decisión\",\n",
    "        model=DecisionTreeClassifier(criterion=\"log_loss\", max_depth=50),\n",
    "    ),\n",
    "    StrokePredictionModel(name=\"Naive Bayes\", model=GaussianNB()),\n",
    "    StrokePredictionModel(\n",
    "        name=\"Random Forest\",\n",
    "        model=RandomForestClassifier(\n",
    "            n_estimators=20, criterion=\"log_loss\", max_depth=50\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento, validación y métricas de desempeño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for predictor in predictors:\n",
    "    predictor.model.fit(X_train, y_train)\n",
    "    y_pred = predictor.model.predict(X_test)\n",
    "    predictor.accuracy = accuracy_score(y_test, y_pred)\n",
    "    predictor.precision = precision_score(y_test, y_pred)\n",
    "    predictor.recall = recall_score(y_test, y_pred)\n",
    "    predictor.f1_score = f1_score(y_test, y_pred)\n",
    "    predictor.confusion_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis de resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"name\": [predictor.name for predictor in predictors],\n",
    "        \"accuracy\": [predictor.accuracy for predictor in predictors],\n",
    "        \"precision\": [predictor.precision for predictor in predictors],\n",
    "        \"recall\": [predictor.recall for predictor in predictors],\n",
    "        \"f1_score\": [predictor.f1_score for predictor in predictors],\n",
    "    }\n",
    ").sort_values(\"f1_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo el proceso puede resumirse en las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stroke_prediction_analysis(X, y, scaler, predictors) -> tuple[pd.DataFrame, StrokePredictionModel]:\n",
    "    # scale data\n",
    "    X = scaler.fit_transform(X)\n",
    "    # division in train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=True, random_state=42\n",
    "    )\n",
    "    for predictor in predictors:\n",
    "        predictor.model.fit(X_train, y_train)\n",
    "        y_pred = predictor.model.predict(X_test)\n",
    "        predictor.accuracy = accuracy_score(y_test, y_pred)\n",
    "        predictor.precision = precision_score(y_test, y_pred)\n",
    "        predictor.recall = recall_score(y_test, y_pred)\n",
    "        predictor.f1_score = f1_score(y_test, y_pred)\n",
    "        predictor.confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"name\": [predictor.name for predictor in predictors],\n",
    "            \"accuracy\": [predictor.accuracy for predictor in predictors],\n",
    "            \"precision\": [predictor.precision for predictor in predictors],\n",
    "            \"recall\": [predictor.recall for predictor in predictors],\n",
    "            \"f1_score\": [predictor.f1_score for predictor in predictors],\n",
    "        }\n",
    "    ).sort_values(\"f1_score\", ascending=False), sorted(predictors, key=lambda x: x.f1_score, reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento desbalanceado de diferentes modelos\n",
    "Se utiliza oversampling con [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html). Es una técnica que combate el problema de los conjuntos de datos desequilibrados, donde una clase (la clase minoritaria) tiene muchas menos muestras que la clase mayoritaria. En lugar de simplemente copiar las muestras existentes de la clase minoritaria, SMOTE genera nuevas muestras sintéticas. Lo hace creando puntos nuevos que están en el espacio entre los puntos existentes de la clase minoritaria, basándose en sus vecinos más cercanos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_smote, y_smote = sm.fit_resample(X, y)\n",
    "print(\"Antes\", X.shape, \"despues\", X_smote.shape)\n",
    "print(\"Antes\", y.shape, \"despues\", y_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results, best_predictor = stroke_prediction_analysis(X_smote, y_smote, scaler, predictors)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de confusión para el mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(15, 5)\n",
    "axs[0].set_title(f\"Matriz de confusión para {best_predictor.name}\")\n",
    "sns.heatmap(best_predictor.confusion_matrix, annot=True , cmap='coolwarm' , linewidths = 0.01 , fmt='g', ax=axs[0])\n",
    "axs[1] = df_results.plot.bar(x=\"name\", ax=axs[1], rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_smote)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_smote, cmap=\"viridis\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.title(\"t-SNE Visualization\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_tsne.shape, y_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a calcular diferentes modelos de K-means, cambiando la cantidad de cluster en cada iteración\n",
    "wcss = []\n",
    "for i in range(1, 20):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=i,\n",
    "        init=\"k-means++\",  # Esta forma busca una inicialización que converga más rapido que incializar al azar\n",
    "        max_iter=300,  # El numero maximo de iteraciones\n",
    "        n_init=10,  # Cuanta veces repite la ejecucion del algoritmo hasta que se quede con el mejor caso\n",
    "        random_state=42,\n",
    "    )\n",
    "    kmeans.fit(X_tsne)\n",
    "    wcss.append(kmeans.inertia_)  # SKlearn le llama inercia a WCSS:w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 20), wcss)\n",
    "plt.xlabel(\"Número de Clusters\", fontsize=12)\n",
    "plt.ylabel(\"WCSS(k)\", fontsize=12)\n",
    "plt.xticks(np.arange(0, 20, step=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=4, init=\"k-means++\", max_iter=300, n_init=10, random_state=42\n",
    ")\n",
    "y_kmeans = kmeans.fit_predict(X_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de los clusters\n",
    "plt.figure(figsize=(6, 6)) \n",
    "plt.scatter(X_tsne[y_kmeans == 0, 0], X_tsne[y_kmeans == 0, 1], s = 30, c = \"#ea5545\")\n",
    "plt.scatter(X_tsne[y_kmeans == 1, 0], X_tsne[y_kmeans == 1, 1], s = 30, c = \"#f46a9b\")\n",
    "plt.scatter(X_tsne[y_kmeans == 2, 0], X_tsne[y_kmeans == 2, 1], s = 30, c = \"#ef9b20\")\n",
    "plt.scatter(X_tsne[y_kmeans == 3, 0], X_tsne[y_kmeans == 3, 1], s = 30, c = \"#bdcf32\")\n",
    "\n",
    "# Usar solo las primeras dos dimensiones para la transformación inversa\n",
    "re_scaled_centroids = (kmeans.cluster_centers_ * scaler.scale_[:2]) + scaler.mean_[:2]\n",
    "\n",
    "plt.scatter(re_scaled_centroids[:,0], re_scaled_centroids[:,1], s=50, c=\"black\", label=\"Centroides\")\n",
    "plt.title(\"Cluster de Pacientes\", fontsize=12)\n",
    "plt.xlabel(\"Componente 1\", fontsize=12)\n",
    "plt.ylabel(\"Componente 2\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"stroke\", axis=1).values\n",
    "y = df[\"stroke\"].values\n",
    "sm = SMOTE(random_state=42)\n",
    "X_smote, y_smote = sm.fit_resample(X, y)  # over-sampling due to class imbalance\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_smote, y_smote, test_size=0.2, stratify=y_smote)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    \"\"\"\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    \"\"\"\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "\n",
    "class BinaryNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para clasificación binaria.\n",
    "\n",
    "    Esta red neuronal consiste en múltiples capas totalmente conectadas con normalización por lotes\n",
    "    y funciones de activación ReLU. La capa de salida utiliza una función de activación sigmoide\n",
    "    para producir probabilidades de clasificación binaria.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): El número de características de entrada.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo BinaryNetwork.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): El número de características de entrada.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(input_size),\n",
    "            torch.nn.Linear(input_size, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Paso hacia adelante a través de la red.\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor de entrada de forma (batch_size, input_size).\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor de salida conteniendo probabilidades de clasificación binaria.\n",
    "        \"\"\"\n",
    "        x = self.layers(x)\n",
    "        return torch.sigmoid(x)  # Escalamos los valores entre 0 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se utilizará cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_size = 16\n",
    "kfolds = 3\n",
    "epochs = 4000\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=kfolds, shuffle=True)\n",
    "# performance metrics\n",
    "results = []\n",
    "\n",
    "# progress bar\n",
    "progress_bar = IntProgress(min=0, max=kfolds * epochs)  # instantiate the bar\n",
    "display(progress_bar)  # display the bar\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    train_subsampler = SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = SubsetRandomSampler(test_ids)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "    # Init the neural network\n",
    "    network = BinaryNetwork(input_size=input_size)\n",
    "    network.apply(reset_weights)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Set current loss value\n",
    "        train_loss = 0\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for inputs, targets in train_loader:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Perform forward pass\n",
    "            outputs = network(inputs)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, targets.unsqueeze(1))\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        results.append({\n",
    "            \"fold\": fold,\n",
    "            \"epoch\": epoch,\n",
    "            \"type\": \"train\",\n",
    "            \"loss\": train_loss,\n",
    "        })\n",
    "\n",
    "        # Saving the model\n",
    "        save_path = f\"./model-fold-{fold}.pth\"\n",
    "        torch.save(network.state_dict(), save_path)\n",
    "\n",
    "        # Evaluation for this fold\n",
    "        test_loss = 0\n",
    "        test_predictions = []\n",
    "        test_targets = []\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the test data and generate predictions\n",
    "            for inputs, targets in test_loader:\n",
    "                # Generate outputs\n",
    "                outputs = network(inputs)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "                test_predictions.extend(outputs.round().squeeze().tolist())\n",
    "                test_targets.extend(targets.tolist())\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        results.append({\n",
    "            \"fold\": fold,\n",
    "            \"epoch\": epoch,\n",
    "            \"type\": \"test\",\n",
    "            \"loss\": test_loss\n",
    "        })\n",
    "\n",
    "        # Get test accuracy\n",
    "        results.append({\n",
    "            \"fold\": fold,\n",
    "            \"epoch\": epoch,\n",
    "            \"type\": \"accuracies\",\n",
    "            \"loss\": accuracy_score(test_targets, test_predictions)\n",
    "        })\n",
    "\n",
    "        # print(\n",
    "        #     f\"Época: {epoch}/{epochs}, Costo de Entrenamiento: {train_loss:.4f}, \"\n",
    "        #     f\"Costo de Validación: {test_loss:.4f}, Exactitud en Validación: {test_accuracies[fold][-1]:.4f}\"\n",
    "        # )\n",
    "\n",
    "        progress_bar.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis de resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "g = sns.FacetGrid(data=results_df[results_df[\"type\"] != \"accuracies\"], col=\"fold\", hue=\"type\")\n",
    "g.map_dataframe(sns.lineplot, x=\"epoch\", y=\"loss\")\n",
    "g.fig.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle(\"Costo durante entrenamiento\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(5,5), ncols=2)\n",
    "# Evaluacion en el conjunto de prueba\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    valid_outputs = network(X_valid_tensor)\n",
    "    valid_outputs = (valid_outputs >= 0.5).float()\n",
    "    valid_accuracy = accuracy_score(y_valid_tensor, valid_outputs)\n",
    "    print(f'Test Accuracy: {valid_accuracy:.4f}')\n",
    "    print(f'F1-score: {f1_score(y_valid_tensor, valid_outputs):.4f}')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_valid_tensor, valid_outputs)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=axs[0])\n",
    "    svc_disp = RocCurveDisplay.from_predictions(y_valid, y_valid_tensor, ax=axs[1])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
